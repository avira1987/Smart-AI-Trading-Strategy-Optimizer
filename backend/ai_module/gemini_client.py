"""
Google Gemini AI client for strategy analysis and parsing
"""

import json
import hashlib
import logging
import time
from collections import deque
from threading import Lock
from typing import Dict, List, Any, Optional, Callable
from pathlib import Path
from django.conf import settings

logger = logging.getLogger(__name__)

from .provider_manager import get_provider_manager

# Constants & configuration defaults
CACHE_TTL_SECONDS = 60 * 60 * 24  # 24 hours
RATE_LIMIT_CALLS_PER_MINUTE = 60
RATE_LIMIT_WINDOW_SECONDS = 60
MAX_INPUT_TOKENS = 32000
MAX_OUTPUT_TOKENS = 8000
DISABLED_MESSAGE = "AI analysis unavailable. Please configure your AI provider (OpenAI ChatGPT or Gemini) in Settings."
SERVICE_UNAVAILABLE_MESSAGE = "AI service temporarily unavailable."
JSON_ONLY_SYSTEM_PROMPT = (
    "You are an assistant that must respond with strictly valid JSON output. "
    "Do not include explanations, markdown fences, or any text outside the JSON."
)

_rate_lock = Lock()
_request_timestamps: deque[float] = deque()

# Cache directory
_CACHE_DIR = Path(getattr(settings, 'CACHE_DIR', Path(__file__).parent.parent / 'cache' / 'gemini'))
_CACHE_DIR.mkdir(parents=True, exist_ok=True)

# System instructions
ANALYSIS_SYSTEM_INSTRUCTIONS = (
    "Ø´Ù…Ø§ ÛŒÚ© ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ù‡Ø³ØªÛŒØ¯. Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ú©Ù‡ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ØŒ "
    "ÛŒÚ© ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯ Ú©Ù‡ Ø´Ø§Ù…Ù„ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø¨Ø§Ø´Ø¯:\n"
    "1. Ø®Ù„Ø§ØµÙ‡ Ú©Ù„ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ\n"
    "2. Ù†Ù‚Ø§Ø· Ù‚ÙˆØª Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ (Ù„ÛŒØ³Øª)\n"
    "3. Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ (Ù„ÛŒØ³Øª)\n"
    "4. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÛŒØ³Ú©\n"
    "5. Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ (Ù„ÛŒØ³Øª)\n"
    "6. Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª (0-100)\n\n"
    "Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§ÛŒØ¯ ÛŒÚ© JSON Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø²ÛŒØ± Ø¨Ø§Ø´Ø¯:\n"
    '{"summary": "...", "strengths": [...], "weaknesses": [...], '
    '"risk_assessment": "...", "recommendations": [...], "quality_score": Ø¹Ø¯Ø¯}'
)

BACKTEST_ANALYSIS_SYSTEM_INSTRUCTIONS = (
    "Ø´Ù…Ø§ ÛŒÚ© ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¨Ú©â€ŒØªØ³Øª Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ù‡Ø³ØªÛŒØ¯. Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ØªØ§ÛŒØ¬ Ø¨Ú©â€ŒØªØ³Øª Ùˆ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡ØŒ "
    "ÛŒÚ© ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ùˆ Ù…ÙØµÙ„ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.\n\n"
    "ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø¨Ø§Ø´Ø¯:\n"
    "1. ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ù„ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ: Ú†Ù‚Ø¯Ø± Ø³ÙˆØ¯ ÛŒØ§ Ø¶Ø±Ø± Ú©Ø±Ø¯Ù‡ Ø§Ø³ØªØŸ\n"
    "2. ØªØ­Ù„ÛŒÙ„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª: Ú†Ù†Ø¯ Ù…Ø¹Ø§Ù…Ù„Ù‡ Ø¨Ø±Ù†Ø¯Ù‡/Ø¨Ø§Ø²Ù†Ø¯Ù‡ Ø¯Ø§Ø´ØªØŸ Ù†Ø±Ø® Ø¨Ø±Ø¯ Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ\n"
    "3. ØªØ­Ù„ÛŒÙ„ Ù‡Ø± Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ: Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø´Ø±Ø· ÙˆØ±ÙˆØ¯/Ø®Ø±ÙˆØ¬ Ú©Ù‡ Ø¯Ø± Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ØŒ Ø¨Ú¯Ùˆ Ú©Ù‡:\n"
    "   - Ø§ÛŒÙ† Ø´Ø±Ø· Ú†Ù†Ø¯ Ø¨Ø§Ø± ÙØ¹Ø§Ù„ Ø´Ø¯Ù‡ Ø§Ø³ØªØŸ\n"
    "   - Ú†Ù‚Ø¯Ø± Ø³ÙˆØ¯Ø¢ÙˆØ± Ø¨ÙˆØ¯Ù‡ Ø§Ø³ØªØŸ\n"
    "   - Ø¢ÛŒØ§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ\n"
    "4. Ù†Ù‚Ø§Ø· Ù‚ÙˆØª Ùˆ Ø¶Ø¹Ù Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ØªØ§ÛŒØ¬ ÙˆØ§Ù‚Ø¹ÛŒ\n"
    "5. Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯\n\n"
    "ØªØ­Ù„ÛŒÙ„ Ø¨Ø§ÛŒØ¯ Ø¯Ù‚ÛŒÙ‚ØŒ Ø¬Ø§Ù…Ø¹ Ùˆ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¨Ø§Ø´Ø¯."
)


def _hash_text(text: str) -> str:
    """Create hash for caching"""
    return hashlib.sha256(text.encode('utf-8')).hexdigest()


def truncate_text(text: str, max_tokens: int = MAX_INPUT_TOKENS) -> str:
    """Truncate text to approximate token limit (4 chars â‰ˆ 1 token)."""
    if not text:
        return text
    max_chars = max_tokens * 4
    if len(text) <= max_chars:
        return text
    logger.debug("Truncating text from %s to %s characters", len(text), max_chars)
    return text[:max_chars]


def _clean_response_text(response_text: str) -> str:
    """Remove code fences and trim whitespace from LLM responses."""
    if not response_text:
        return ""
    cleaned = response_text.strip()
    if cleaned.startswith("```"):
        cleaned = cleaned.strip("`")
        parts = cleaned.split('\n', 1)
        cleaned = parts[1] if len(parts) > 1 else parts[0]
    return cleaned.strip()


def _providers_available() -> bool:
    manager = get_provider_manager()
    return manager.has_available_provider()


def _build_base_response(
    ai_status: str,
    message: str,
    *,
    raw_output: str = "",
    extra: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """Create a standardized response payload."""
    payload: Dict[str, Any] = {
        "entry_conditions": [],
        "exit_conditions": [],
        "risk_management": {},
        "ai_status": ai_status,
        "message": message,
        "raw_output": raw_output or "",
    }
    if extra:
        for key, value in extra.items():
            payload[key] = value
    return payload


def _translate_ai_error_message(error_message: Optional[str]) -> str:
    if not error_message:
        return SERVICE_UNAVAILABLE_MESSAGE

    message = error_message.strip()
    lowered = message.lower()

    if "user location is not supported" in lowered:
        return "Ø³Ø±ÙˆÛŒØ³ Gemini Ø¯Ø± Ù…ÙˆÙ‚Ø¹ÛŒØª Ù…Ú©Ø§Ù†ÛŒ ÙØ¹Ù„ÛŒ Ø´Ù…Ø§ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§Ù‹ Ø§Ø² VPN ÛŒØ§ Ø§Ø±Ø§Ø¦Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯."
    if "location" in lowered and "not supported" in lowered:
        return "Ø³Ø±ÙˆÛŒØ³ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…ÙˆÙ‚Ø¹ÛŒØª Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ ÙØ¹Ø§Ù„ Ù†ÛŒØ³Øª."
    if "api key" in lowered and "invalid" in lowered:
        return "Ú©Ù„ÛŒØ¯ Gemini Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ú©Ù„ÛŒØ¯ Ø¬Ø¯ÛŒØ¯ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯."
    if "permission" in lowered and "denied" in lowered:
        return "Ø¯Ø³ØªØ±Ø³ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³Ø±ÙˆÛŒØ³ Gemini ÙØ±Ø§Ù‡Ù… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª."
    if "quota" in lowered or "exceeded" in lowered or "rate limit" in lowered:
        return "Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ù…ØµØ±Ù Ø³Ø±ÙˆÛŒØ³ Gemini ØªÙ…Ø§Ù… Ø´Ø¯Ù‡ Ø§Ø³Øª. Ú©Ù…ÛŒ Ø¨Ø¹Ø¯ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯."
    if "model" in lowered and "not found" in lowered:
        return "Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ Gemini Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§Ù‹ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú©Ù†ÛŒØ¯."

    return message


def _cache_file(namespace: str, digest: str) -> Path:
    """Return cache file path for a given namespace and digest."""
    namespace_dir = _CACHE_DIR / namespace
    namespace_dir.mkdir(parents=True, exist_ok=True)
    return namespace_dir / f"{digest}.json"


def _load_cache(namespace: str, digest: str) -> Optional[Dict[str, Any]]:
    """Load cached response if still within TTL."""
    path = _cache_file(namespace, digest)
    if not path.exists():
        return None

    try:
        data = json.loads(path.read_text(encoding='utf-8'))
        timestamp = data.get("timestamp")
        payload = data.get("payload")
        if not timestamp or payload is None:
            return None
        if time.time() - float(timestamp) > CACHE_TTL_SECONDS:
            try:
                path.unlink(missing_ok=True)
            except Exception:
                pass
            return None
        return payload
    except Exception as exc:
        logger.debug("Failed to load AI cache: %s", exc, exc_info=True)
        return None


def _write_cache(namespace: str, digest: str, payload: Dict[str, Any]) -> None:
    """Write payload to cache with timestamp."""
    path = _cache_file(namespace, digest)
    try:
        path.write_text(
            json.dumps({"timestamp": time.time(), "payload": payload}, ensure_ascii=False),
            encoding='utf-8'
        )
    except Exception as exc:
        logger.debug("Failed to write AI cache: %s", exc, exc_info=True)


def _enforce_rate_limit() -> Optional[Dict[str, Any]]:
    """Enforce per-minute rate limit. Returns error response if limit exceeded."""
    if RATE_LIMIT_CALLS_PER_MINUTE <= 0:
        return None

    now = time.time()
    with _rate_lock:
        while _request_timestamps and now - _request_timestamps[0] > RATE_LIMIT_WINDOW_SECONDS:
            _request_timestamps.popleft()
        if len(_request_timestamps) >= RATE_LIMIT_CALLS_PER_MINUTE:
            logger.warning("AI provider rate limit exceeded (%s per minute)", RATE_LIMIT_CALLS_PER_MINUTE)
            return _build_base_response(
                ai_status="error",
                message=SERVICE_UNAVAILABLE_MESSAGE,
                raw_output="",
                extra={"error": "Rate limit exceeded"}
            )
        _request_timestamps.append(now)
    return None


def _get_gemini_api_key() -> Optional[str]:
    """Compatibility helper to fetch Gemini API key."""
    provider = get_provider_manager().providers.get("gemini")
    if provider:
        key = provider.get_api_key()
        if key:
            return key
    return getattr(settings, 'GEMINI_API_KEY', '') or ''


def _call_gemini(
    prompt: str,
    *,
    cache_namespace: str,
    cache_key: str,
    generation_config: Optional[Dict[str, Any]],
    response_parser: Callable[[str], Dict[str, Any]]
) -> Dict[str, Any]:
    """Execute AI provider call with caching, rate limiting, and standardized responses."""
    manager = get_provider_manager()
    digest = _hash_text(cache_key)
    cached = _load_cache(cache_namespace, digest)
    if cached:
        return cached

    if not manager.has_available_provider():
        return _build_base_response(
            ai_status="disabled",
            message=DISABLED_MESSAGE,
            extra={"error": "no_provider_available"}
        )

    rate_limit_error = _enforce_rate_limit()
    if rate_limit_error:
        return rate_limit_error

    config = dict(generation_config or {})
    configured_max_tokens = getattr(settings, 'GEMINI_MAX_OUTPUT_TOKENS', MAX_OUTPUT_TOKENS)
    config.setdefault('max_output_tokens', min(configured_max_tokens, MAX_OUTPUT_TOKENS))
    metadata = config.pop('provider_metadata', None) or {}

    result = manager.generate(prompt, config, metadata=metadata)

    attempts_serialized = [
        {
            "provider": attempt.provider,
            "success": attempt.success,
            "error": attempt.error,
            "status_code": attempt.status_code,
            "latency_ms": attempt.latency_ms,
            "tokens_used": attempt.tokens_used,
        }
        for attempt in result.attempts
    ]

    if not result.success or not result.text:
        logger.warning(
            "AI providers failed to generate response: %s",
            result.error or "unknown_error",
        )
        error_text = result.error or SERVICE_UNAVAILABLE_MESSAGE
        user_message = _translate_ai_error_message(error_text)
        extra = {
            "error": error_text,
            "translated_error": user_message,
            "provider_attempts": attempts_serialized,
        }
        return _build_base_response(
            ai_status="error",
            message=user_message,
            extra=extra,
        )

    raw_output = result.text

    cleaned_output = _clean_response_text(raw_output)

    try:
        parsed_response = response_parser(cleaned_output)
    except Exception as exc:
        logger.warning("AI response parsing failed: %s", exc, exc_info=True)
        parsed_response = _build_base_response(
            ai_status="error",
            message=SERVICE_UNAVAILABLE_MESSAGE,
            raw_output=cleaned_output,
            extra={"error": str(exc)}
        )

    if "raw_output" not in parsed_response:
        parsed_response["raw_output"] = cleaned_output
    parsed_response.setdefault("provider_attempts", attempts_serialized)
    if result.provider:
        parsed_response.setdefault("provider", result.provider)
        parsed_response.setdefault("ai_provider", result.provider)
    if result.tokens_used is not None:
        parsed_response.setdefault("tokens_used", result.tokens_used)

    if parsed_response.get("ai_status") == "ok":
        _write_cache(cache_namespace, digest, parsed_response)

    return parsed_response


def parse_with_gemini(text: str) -> Dict[str, Any]:
    """Parse strategy text using AI providers with standardized response."""
    truncated_text = truncate_text(text or "")
    cache_key = truncated_text or "empty"

    def _parse_response(raw_output: str) -> Dict[str, Any]:
        if not raw_output:
            return _build_base_response(
                ai_status="error",
                message=SERVICE_UNAVAILABLE_MESSAGE,
                raw_output=raw_output,
                extra={"error": "empty_response"}
            )

        try:
            data = json.loads(raw_output)
        except json.JSONDecodeError as exc:
            logger.warning("Failed to decode AI parse response: %s", exc)
            return _build_base_response(
                ai_status="error",
                message=SERVICE_UNAVAILABLE_MESSAGE,
                raw_output=raw_output,
                extra={"error": str(exc)}
            )

        entry_conditions = data.get("entry_conditions") or []
        exit_conditions = data.get("exit_conditions") or []
        risk_management = data.get("risk_management") or {}

        return _build_base_response(
            ai_status="ok",
            message="AI parsing successful.",
            raw_output=raw_output,
            extra={
                "entry_conditions": entry_conditions,
                "exit_conditions": exit_conditions,
                "risk_management": risk_management,
                "indicators": data.get("indicators", []),
                "timeframe": data.get("timeframe"),
                "symbol": data.get("symbol"),
                "confidence": data.get("confidence"),
                "source": "llm",
            }
        )

    prompt = f"""
    Ø§ÛŒÙ† ÛŒÚ© Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ ÛŒØ§ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª. 
    Ù„Ø·ÙØ§Ù‹ Ø¢Ù† Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒØ¯ Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ± Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒØ¯:
    
    {truncated_text}
    
    Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§ÛŒØ¯ JSON Ø¨Ø§ Ø§ÛŒÙ† Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø§Ø´Ø¯:
    {{
        "entry_conditions": ["Ø´Ø±Ø· 1", "Ø´Ø±Ø· 2"],
        "exit_conditions": ["Ø´Ø±Ø· 1", "Ø´Ø±Ø· 2"],
        "indicators": ["RSI", "MACD"],
        "risk_management": {{"stop_loss": 50, "take_profit": 100}},
        "timeframe": "H1",
        "symbol": "EURUSD"
    }}
    
    ÙÙ‚Ø· JSON Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯.
    """

    return _call_gemini(
        prompt,
        cache_namespace="parse",
        cache_key=cache_key,
        generation_config={
            'temperature': 0.3,
            'response_mime_type': 'application/json',
            'provider_metadata': {'system_prompt': JSON_ONLY_SYSTEM_PROMPT},
        },
        response_parser=_parse_response
    )


def call_gemini_analyzer(text: str) -> Dict[str, Any]:
    """Public helper used by tests to parse strategy text via Gemini."""
    return parse_with_gemini(text)


def generate_basic_analysis(parsed_strategy: Dict[str, Any]) -> Dict[str, Any]:
    """Generate a basic analysis without AI - fallback when Gemini is not available"""
    entry_conditions = parsed_strategy.get('entry_conditions', [])
    exit_conditions = parsed_strategy.get('exit_conditions', [])
    risk_management = parsed_strategy.get('risk_management', {})
    indicators = parsed_strategy.get('indicators', [])
    
    summary = f"Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø´Ø§Ù…Ù„ {len(entry_conditions)} Ø´Ø±Ø· ÙˆØ±ÙˆØ¯ Ùˆ {len(exit_conditions)} Ø´Ø±Ø· Ø®Ø±ÙˆØ¬ Ø§Ø³Øª."
    if indicators:
        summary += f" Ø§Ø² Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ {', '.join(indicators)} Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."
    
    strengths = []
    if entry_conditions:
        strengths.append("Ø¯Ø§Ø±Ø§ÛŒ Ø´Ø±Ø§ÛŒØ· ÙˆØ±ÙˆØ¯ Ù…Ø´Ø®Øµ Ø§Ø³Øª")
    if exit_conditions:
        strengths.append("Ø¯Ø§Ø±Ø§ÛŒ Ø´Ø±Ø§ÛŒØ· Ø®Ø±ÙˆØ¬ Ù…Ø´Ø®Øµ Ø§Ø³Øª")
    if risk_management:
        strengths.append("Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒØ³Ú© ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ø³Øª")
    
    weaknesses = []
    if not entry_conditions:
        weaknesses.append("Ø´Ø±Ø§ÛŒØ· ÙˆØ±ÙˆØ¯ Ù…Ø´Ø®Øµ Ù†ÛŒØ³Øª")
    if not exit_conditions:
        weaknesses.append("Ø´Ø±Ø§ÛŒØ· Ø®Ø±ÙˆØ¬ Ù…Ø´Ø®Øµ Ù†ÛŒØ³Øª")
    if not risk_management:
        weaknesses.append("Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒØ³Ú© Ú©Ø§Ù…Ù„ Ù†ÛŒØ³Øª")
    
    risk_assessment = "Ø±ÛŒØ³Ú© Ù…ØªÙˆØ³Ø·"
    if not risk_management.get('stop_loss'):
        risk_assessment = "Ø±ÛŒØ³Ú© Ø¨Ø§Ù„Ø§ - Ø­Ø¯ Ø¶Ø±Ø± ØªØ¹Ø±ÛŒÙ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª"
    
    recommendations = []
    if not risk_management.get('stop_loss'):
        recommendations.append("ØªØ¹Ø±ÛŒÙ Ø­Ø¯ Ø¶Ø±Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒØ³Ú©")
    if len(entry_conditions) < 2:
        recommendations.append("Ø§ÙØ²ÙˆØ¯Ù† Ø´Ø±Ø§ÛŒØ· ÙˆØ±ÙˆØ¯ Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª")
    
    quality_score = 50
    if entry_conditions and exit_conditions and risk_management:
        quality_score = 70
    if len(entry_conditions) > 2 and len(exit_conditions) > 1:
        quality_score = 80
    
    return {
        "summary": summary,
        "strengths": strengths,
        "weaknesses": weaknesses,
        "risk_assessment": risk_assessment,
        "recommendations": recommendations,
        "quality_score": quality_score / 100.0,
        "is_basic": True
    }


def analyze_strategy_with_gemini(parsed_strategy: Dict[str, Any], raw_text: str = None, user=None) -> Dict[str, Any]:
    """Generate comprehensive analysis of a trading strategy using Gemini AI."""
    # Create a text description of the strategy for analysis
    strategy_description = f"""
Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ:
- Ø´Ø±Ø§ÛŒØ· ÙˆØ±ÙˆØ¯: {', '.join(parsed_strategy.get('entry_conditions', []))}
- Ø´Ø±Ø§ÛŒØ· Ø®Ø±ÙˆØ¬: {', '.join(parsed_strategy.get('exit_conditions', []))}
- Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒØ³Ú©: {parsed_strategy.get('risk_management', {})}
- Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§: {', '.join(parsed_strategy.get('indicators', []))}
- Ù†Ù…Ø§Ø¯: {parsed_strategy.get('symbol', 'ØªØ¹ÛŒÛŒÙ† Ù†Ø´Ø¯Ù‡')}
- ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…: {parsed_strategy.get('timeframe', 'ØªØ¹ÛŒÛŒÙ† Ù†Ø´Ø¯Ù‡')}
- Ø§Ù…ØªÛŒØ§Ø² Ø§Ø¹ØªÙ…Ø§Ø¯: {parsed_strategy.get('confidence_score', 0.0) * 100:.0f}%
"""

    if raw_text:
        strategy_description += f"\nÙ…ØªÙ† Ø§ØµÙ„ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ:\n{truncate_text(raw_text, max_tokens=8000)[:8000*4]}"

    cache_key = strategy_description

    prompt = (
        f"{ANALYSIS_SYSTEM_INSTRUCTIONS}\n\n"
        f"{strategy_description}\n\n"
        f"Ù„Ø·ÙØ§Ù‹ ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯."
    )

    def _parse_response(raw_output: str) -> Dict[str, Any]:
        if not raw_output:
            return _build_base_response(
                ai_status="error",
                message=SERVICE_UNAVAILABLE_MESSAGE,
                raw_output=raw_output,
                extra={"error": "empty_response"}
            )

        try:
            data: Dict[str, Any] = json.loads(raw_output)
        except json.JSONDecodeError as exc:
            logger.warning("Failed to decode Gemini strategy analysis response: %s", exc)
            return _build_base_response(
                ai_status="error",
                message=SERVICE_UNAVAILABLE_MESSAGE,
                raw_output=raw_output,
                extra={"error": str(exc)}
            )

        strengths = data.get('strengths') or []
        weaknesses = data.get('weaknesses') or []
        recommendations = data.get('recommendations') or []
        risk_assessment = data.get('risk_assessment') or 'Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÛŒØ³Ú© Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.'
        summary = data.get('summary') or 'ØªØ­Ù„ÛŒÙ„ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.'
        quality_score = data.get('quality_score', 50)

        if isinstance(quality_score, (int, float)):
            if quality_score > 1:
                quality_score = float(quality_score) / 100.0
        else:
            quality_score = 0.5

        analysis_payload = {
            "summary": summary,
            "strengths": strengths,
            "weaknesses": weaknesses,
            "risk_assessment": risk_assessment,
            "recommendations": recommendations,
            "quality_score": quality_score,
            "is_basic": False,
            "source": "llm",
        }

        return _build_base_response(
            ai_status="ok",
            message="AI analysis successful.",
            raw_output=raw_output,
            extra=analysis_payload
        )

    result = _call_gemini(
        prompt,
        cache_namespace="analysis",
        cache_key=cache_key,
        generation_config={
            'temperature': 0.7,
            'response_mime_type': 'application/json',
            'provider_metadata': {'system_prompt': JSON_ONLY_SYSTEM_PROMPT},
        },
        response_parser=_parse_response
    )

    provider_name = result.get("ai_provider") or result.get("provider") or "ai"

    if result.get("ai_status") == "ok":
        try:
            from api.api_usage_tracker import log_api_usage

            input_tokens_approx = len(prompt) // 4
            output_tokens_approx = len(result.get("raw_output", "")) // 4
            total_tokens = input_tokens_approx + output_tokens_approx

            metadata = {
                'function': 'analyze_strategy_with_gemini',
                'input_tokens_approx': input_tokens_approx,
                'output_tokens_approx': output_tokens_approx,
                'total_tokens_approx': total_tokens,
                'provider_attempts': result.get("provider_attempts"),
            }

            log_api_usage(
                provider=provider_name,
                endpoint='analyze_strategy_with_gemini',
                request_type='POST',
                status_code=200,
                success=True,
                tokens=total_tokens,
                user=user,
                metadata=metadata,
            )
        except Exception as log_error:
            logger.warning("Failed to log AI usage: %s", log_error)
    elif result.get("ai_status") == "error":
        try:
            from api.api_usage_tracker import log_api_usage
            metadata = {
                'function': 'analyze_strategy_with_gemini',
                'error': result.get("error"),
                'provider_attempts': result.get("provider_attempts"),
            }
            log_api_usage(
                provider=provider_name,
                endpoint='analyze_strategy_with_gemini',
                request_type='POST',
                status_code=500,
                success=False,
                error_message=result.get("error"),
                user=user,
                metadata=metadata,
            )
        except Exception:
            pass

    return result


def generate_strategy_questions(
    parsed_strategy: Dict[str, Any],
    raw_text: str,
    existing_answers: Dict[str, Any] = None
) -> Dict[str, Any]:
    """Generate intelligent follow-up questions for completing a strategy."""
    logger.info("Starting question generation...")

    existing_answers = existing_answers or {}

    summary = (
        f"Ø´Ø±Ø§ÛŒØ· ÙˆØ±ÙˆØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {len(parsed_strategy.get('entry_conditions', []))} Ø´Ø±Ø·\n"
        f"Ø´Ø±Ø§ÛŒØ· Ø®Ø±ÙˆØ¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {len(parsed_strategy.get('exit_conditions', []))} Ø´Ø±Ø·\n"
        f"Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§: {', '.join(parsed_strategy.get('indicators', []))}\n"
        f"Ø§Ù…ØªÛŒØ§Ø² Ø§Ø¹ØªÙ…Ø§Ø¯: {parsed_strategy.get('confidence_score', 0) * 100:.0f}%"
    )

    if existing_answers:
        summary += f"\nØ¬ÙˆØ§Ø¨â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ú©Ø§Ø±Ø¨Ø±:\n{json.dumps(existing_answers, ensure_ascii=False, indent=2)}"

    truncated_text = truncate_text(raw_text or "", max_tokens=MAX_INPUT_TOKENS // 2)
    cache_key = f"{summary}\n{truncated_text}"

    prompt = f"""
    Ø´Ù…Ø§ ÛŒÚ© ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ù‡Ø³ØªÛŒØ¯. Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ú©Ù‡ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ØŒ Ø¨Ø§ÛŒØ¯ Ø³ÙˆØ§Ù„Ø§Øª Ù‡ÙˆØ´Ù…Ù†Ø¯Ø§Ù†Ù‡ Ùˆ Ù‡Ø¯ÙÙ…Ù†Ø¯ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø± Ú©Ù…Ú© Ú©Ù†Ø¯ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø±Ø§ Ú©Ø§Ù…Ù„â€ŒØªØ± Ùˆ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± ØªØ¹Ø±ÛŒÙ Ú©Ù†Ø¯.
    
    Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ:
    {summary}
    
    Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ (Ù‚Ø³Ù…Øª Ø§ÙˆÙ„):
    {truncated_text}
    
    **Ù‚ÙˆØ§Ù†ÛŒÙ† Ù…Ù‡Ù… Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø³ÙˆØ§Ù„Ø§Øª:**
    
    1. Ù‚Ø¨Ù„ Ø§Ø² ØªÙˆÙ„ÛŒØ¯ Ù‡Ø± Ø³ÙˆØ§Ù„ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ø¢ÛŒØ§ Ù¾Ø§Ø³Ø® Ø¢Ù† Ø³ÙˆØ§Ù„ Ø¯Ø± Ù…ØªÙ† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª ÛŒØ§ Ù†Ù‡.
       - Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø³ÙˆØ§Ù„ Ø¨Ù‡ Ø·ÙˆØ± ÙˆØ§Ø¶Ø­ Ùˆ Ú©Ø§Ù…Ù„ Ø¯Ø± Ù…ØªÙ† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø¢Ù…Ø¯Ù‡ Ø§Ø³ØªØŒ Ø¢Ù† Ø³ÙˆØ§Ù„ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù†Ú©Ù†ÛŒØ¯.
       - ÙÙ‚Ø· Ø³ÙˆØ§Ù„Ø§ØªÛŒ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ù¾Ø§Ø³Ø®Ø´Ø§Ù† Ø¯Ø± Ù…ØªÙ† Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª ÛŒØ§ Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø¨Ù‡Ù… Ø¨ÛŒØ§Ù† Ø´Ø¯Ù‡ Ø§Ø³Øª.
    
    2. Ù‡Ø¯Ù Ø´Ù…Ø§: ØªÙˆÙ„ÛŒØ¯ 3 ØªØ§ 5 Ø³ÙˆØ§Ù„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ú©Ù‡:
       - Ù†Ù‚Ø§Ø· Ù…Ø¨Ù‡Ù… Ùˆ Ù†Ø§Ù‚Øµ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø±Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ù†Ù†Ø¯.
       - Ø´Ø±Ø§ÛŒØ· ÙˆØ±ÙˆØ¯/Ø®Ø±ÙˆØ¬ Ø±Ø§ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ú©Ù†Ù†Ø¯ Ø§Ú¯Ø± Ù…Ø¨Ù‡Ù… Ø§Ø³Øª.
       - Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ù‡Ù… (Ø­Ø¯ Ø¶Ø±Ø±ØŒ Ø­Ø¯ Ø³ÙˆØ¯ØŒ ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…) Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†Ù†Ø¯ Ø§Ú¯Ø± Ø°Ú©Ø± Ù†Ø´Ø¯Ù‡.
       - Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†Ù†Ø¯ Ø§Ú¯Ø± Ø°Ú©Ø± Ù†Ø´Ø¯Ù‡.
    
    3. Ù…Ø«Ø§Ù„:
       - Ø§Ú¯Ø± Ø¯Ø± Ù…ØªÙ† Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡ Â«Ø­Ø¯ Ø¶Ø±Ø± 50 Ù¾ÛŒÙ¾ Ø§Ø³ØªÂ»ØŒ Ø³ÙˆØ§Ù„ Â«Ø­Ø¯ Ø¶Ø±Ø± Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸÂ» Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù†Ú©Ù†ÛŒØ¯.
       - Ø§Ú¯Ø± Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡ Â«Ø§Ø² Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± RSI Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…Â»ØŒ Ø³ÙˆØ§Ù„ Â«Ú©Ø¯Ø§Ù… Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸÂ» Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù†Ú©Ù†ÛŒØ¯.
    
    Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§ÛŒØ¯ JSON Ø¨Ø§ Ø§ÛŒÙ† Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø§Ø´Ø¯:
    {{
      "questions": [
        {{
          "question_text": "Ù…ØªÙ† Ø³ÙˆØ§Ù„ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ",
          "question_type": "text|number|choice|multiple_choice|boolean",
          "options": ["Ú¯Ø²ÛŒÙ†Ù‡ 1", "Ú¯Ø²ÛŒÙ†Ù‡ 2"],
          "order": 1,
          "context": {{
            "section": "entry|exit|risk|indicator",
            "related_text": "Ø¨Ø®Ø´ÛŒ Ø§Ø² Ù…ØªÙ† Ú©Ù‡ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø§Ø³Øª"
          }}
        }}
      ]
    }}
    
    ÙÙ‚Ø· JSON Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯ Ùˆ Ø§Ø² ØªÙˆØ¶ÛŒØ­Ø§Øª Ø§Ø¶Ø§ÙÙ‡ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯.
    """

    def _parse_response(raw_output: str) -> Dict[str, Any]:
        if not raw_output:
            return _build_base_response(
                ai_status="error",
                message=SERVICE_UNAVAILABLE_MESSAGE,
                raw_output=raw_output,
                extra={"questions": []}
            )

        try:
            data: Dict[str, Any] = json.loads(raw_output)
        except json.JSONDecodeError as exc:
            logger.warning("Failed to parse questions JSON: %s", exc)
            return _build_base_response(
                ai_status="error",
                message=SERVICE_UNAVAILABLE_MESSAGE,
                raw_output=raw_output,
                extra={"questions": [], "error": str(exc)}
            )

        questions = data.get("questions") or []
        if not isinstance(questions, list):
            logger.warning("Gemini questions response missing list, got %s", type(questions))
            return _build_base_response(
                ai_status="error",
                message=SERVICE_UNAVAILABLE_MESSAGE,
                raw_output=raw_output,
                extra={"questions": [], "error": "invalid_structure"}
            )

        return _build_base_response(
            ai_status="ok",
            message="AI question generation successful.",
            raw_output=raw_output,
            extra={
                "questions": questions,
                "questions_count": len(questions)
            }
        )

    result = _call_gemini(
        prompt,
        cache_namespace="questions",
        cache_key=cache_key,
        generation_config={
            'temperature': 0.7,
            'response_mime_type': 'application/json',
            'provider_metadata': {'system_prompt': JSON_ONLY_SYSTEM_PROMPT},
        },
        response_parser=_parse_response
    )

    logger.info("Question generation completed with status: %s", result.get("ai_status"))
    return result


def parse_strategy_with_answers(
    parsed_strategy: Dict[str, Any],
    raw_text: str,
    answers: Dict[str, Any]
) -> Dict[str, Any]:
    """ØªØ¨Ø¯ÛŒÙ„ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø¨Ù‡ Ù…Ø¯Ù„ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¬ÙˆØ§Ø¨â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø± Ùˆ Gemini"""
    if not _providers_available():
        logger.warning("AI provider not available for strategy conversion")
        return parsed_strategy
    
    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø±Ø§ÛŒ Gemini
    strategy_info = f"""
    Ø´Ø±Ø§ÛŒØ· ÙˆØ±ÙˆØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡:
    {chr(10).join(f'- {c}' for c in parsed_strategy.get('entry_conditions', []))}
    
    Ø´Ø±Ø§ÛŒØ· Ø®Ø±ÙˆØ¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡:
    {chr(10).join(f'- {c}' for c in parsed_strategy.get('exit_conditions', []))}
    
    Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§: {', '.join(parsed_strategy.get('indicators', []))}
    Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒØ³Ú©: {json.dumps(parsed_strategy.get('risk_management', {}), ensure_ascii=False)}
    """
    
    answers_text = f"""
    Ø¬ÙˆØ§Ø¨â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±:
    {json.dumps(answers, ensure_ascii=False, indent=2)}
    """
    
    prompt = f"""
    Ø´Ù…Ø§ ÛŒÚ© Ù…Ø¨Ø¯Ù„ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ù‡Ø³ØªÛŒØ¯. Ø¨Ø§ÛŒØ¯ ÛŒÚ© Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù…ØªÙ†ÛŒ (ÙØ§Ø±Ø³ÛŒ/Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ) Ø±Ø§ Ø¨Ù‡ ÛŒÚ© Ù…Ø¯Ù„ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒØ¯.
    
    Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ:
    {strategy_info}
    
    {answers_text}
    
    Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ:
    {raw_text[:4000]}
    
    Ù‡Ø¯Ù: ØªØ¨Ø¯ÛŒÙ„ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø¨Ù‡ ÛŒÚ© Ø³Ø§Ø®ØªØ§Ø± JSON Ú©Ø§Ù…Ù„ Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ú©Ù‡ Ø´Ø§Ù…Ù„:
    1. Ø´Ø±Ø§ÛŒØ· ÙˆØ±ÙˆØ¯ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§
    2. Ø´Ø±Ø§ÛŒØ· Ø®Ø±ÙˆØ¬ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§
    3. Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚
    4. Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒØ³Ú© Ú©Ø§Ù…Ù„
    5. ÛŒÚ© Ù…Ø¯Ù„ Ø§Ø¬Ø±Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨ØªÙˆØ§Ù†Ø¯ Ø¨Ø§ Ø¢Ù† ØªØ±ÛŒØ¯ Ú©Ù†Ø¯
    
    Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§ÛŒØ¯ JSON Ø¨Ø§ Ø§ÛŒÙ† Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø§Ø´Ø¯:
    {{
      "entry_conditions": [
        {{
          "condition": "Ø´Ø±Ø· Ø¨Ù‡ ØµÙˆØ±Øª Ù…ØªÙ†",
          "type": "indicator|price_action|pattern|custom",
          "params": {{}},
          "code_snippet": "Ú©Ø¯ Python Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)"
        }}
      ],
      "exit_conditions": [...],
      "indicators": {{
        "rsi": {{"period": 14}},
        "macd": {{"fast": 12, "slow": 26, "signal": 9}}
      }},
      "risk_management": {{
        "stop_loss": {{"type": "pips|percentage|price", "value": 50}},
        "take_profit": {{"type": "pips|percentage|price", "value": 100}},
        "risk_per_trade": 2
      }},
      "executable_model": {{
        "entry_logic": "Ù…Ù†Ø·Ù‚ ÙˆØ±ÙˆØ¯ Ø¨Ù‡ ØµÙˆØ±Øª Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§",
        "exit_logic": "Ù…Ù†Ø·Ù‚ Ø®Ø±ÙˆØ¬ Ø¨Ù‡ ØµÙˆØ±Øª Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§"
      }}
    }}
    
    ÙÙ‚Ø· JSON Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯.
    """
    
    def _parse_response(raw_output: str) -> Dict[str, Any]:
        if not raw_output:
            return {
                "ai_status": "error",
                "message": SERVICE_UNAVAILABLE_MESSAGE,
                "raw_output": raw_output,
                "error": "empty_response",
            }

        cleaned = _clean_response_text(raw_output)
        try:
            enhanced_strategy: Dict[str, Any] = json.loads(cleaned)
        except json.JSONDecodeError as exc:
            logger.warning("Failed to decode strategy conversion response: %s", exc)
            return {
                "ai_status": "error",
                "message": SERVICE_UNAVAILABLE_MESSAGE,
                "raw_output": cleaned,
                "error": str(exc),
            }

        return {
            "ai_status": "ok",
            "message": "Strategy conversion successful.",
            "raw_output": cleaned,
            "enhanced_strategy": enhanced_strategy,
        }

    result = _call_gemini(
        prompt,
        cache_namespace="strategy_conversion",
        cache_key=cache_key,
        generation_config={
            'temperature': 0.3,
            'response_mime_type': 'application/json',
            'provider_metadata': {'system_prompt': JSON_ONLY_SYSTEM_PROMPT},
        },
        response_parser=_parse_response
    )

    if result.get("ai_status") == "ok":
        enhanced = result.get("enhanced_strategy") or {}
        if isinstance(enhanced, dict):
            merged = dict(parsed_strategy)
            merged.update(enhanced)
            return merged

    return parsed_strategy


def analyze_backtest_trades_with_ai(
    backtest_results: Dict[str, Any],
    strategy: Dict[str, Any],
    symbol: str,
    data_provider: str = None,
    data_points: int = 0,
    date_range: str = None,
    user=None
) -> Dict[str, Any]:
    """Analyze backtest trades using AI (Gemini) and return structured analysis."""
    # Prepare comprehensive backtest data for analysis
    analysis_data = {
        "total_trades": backtest_results.get('total_trades', 0),
        "winning_trades": backtest_results.get('winning_trades', 0),
        "losing_trades": backtest_results.get('losing_trades', 0),
        "win_rate": backtest_results.get('win_rate', 0.0),
        "total_return": backtest_results.get('total_return', 0.0),
        "max_drawdown": backtest_results.get('max_drawdown', 0.0),
        "sharpe_ratio": backtest_results.get('sharpe_ratio', 0.0),
        "profit_factor": backtest_results.get('profit_factor', 0.0),
        "entry_conditions": strategy.get('entry_conditions', []),
        "exit_conditions": strategy.get('exit_conditions', []),
        "risk_management": strategy.get('risk_management', {}),
        "symbol": symbol,
        "sample_trades": (backtest_results.get('trades') or [])[:10],
    }

    if data_provider:
        analysis_data["data_provider"] = data_provider
    if data_points > 0:
        analysis_data["data_points"] = data_points
    if date_range:
        analysis_data["date_range"] = date_range

    cache_key = json.dumps(analysis_data, ensure_ascii=False, sort_keys=True)

    prompt = (
        f"{BACKTEST_ANALYSIS_SYSTEM_INSTRUCTIONS}\n\n"
        f"Ù†ØªØ§ÛŒØ¬ Ø¨Ú©â€ŒØªØ³Øª:\n{json.dumps(analysis_data, ensure_ascii=False, indent=2)}\n\n"
        f"Ù„Ø·ÙØ§Ù‹ ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯."
    )

    def _parse_response(raw_output: str) -> Dict[str, Any]:
        if not raw_output:
            return _build_base_response(
                ai_status="error",
                message=SERVICE_UNAVAILABLE_MESSAGE,
                raw_output=raw_output,
                extra={"analysis_text": ""}
            )

        return _build_base_response(
            ai_status="ok",
            message="Gemini backtest analysis successful.",
            raw_output=raw_output,
            extra={
                "analysis_text": raw_output,
                "analysis_segments": [segment.strip() for segment in raw_output.split("\n\n") if segment.strip()],
                "is_basic": False,
                "source": "llm"
            }
        )

    result = _call_gemini(
        prompt,
        cache_namespace="backtest_analysis",
        cache_key=cache_key,
        generation_config={
            'temperature': 0.7,
            'response_mime_type': 'text/plain',
            'provider_metadata': {'system_prompt': JSON_ONLY_SYSTEM_PROMPT},
        },
        response_parser=_parse_response
    )

    provider_name = result.get("ai_provider") or result.get("provider") or "ai"

    if result.get("ai_status") == "ok":
        try:
            from api.api_usage_tracker import log_api_usage

            input_tokens_approx = len(prompt) // 4
            output_tokens_approx = len(result.get("raw_output", "")) // 4
            total_tokens = input_tokens_approx + output_tokens_approx

            log_api_usage(
                provider=provider_name,
                endpoint='analyze_backtest_trades_with_ai',
                request_type='POST',
                status_code=200,
                success=True,
                tokens=total_tokens,
                user=user,
                metadata={
                    'function': 'analyze_backtest_trades_with_ai',
                    'symbol': symbol,
                    'input_tokens_approx': input_tokens_approx,
                    'output_tokens_approx': output_tokens_approx,
                    'total_tokens_approx': total_tokens,
                    'provider_attempts': result.get("provider_attempts"),
                }
            )
        except Exception as log_error:
            logger.warning("Failed to log AI usage: %s", log_error)
    elif result.get("ai_status") == "error":
        try:
            from api.api_usage_tracker import log_api_usage
            log_api_usage(
                provider=provider_name,
                endpoint='analyze_backtest_trades_with_ai',
                request_type='POST',
                status_code=500,
                success=False,
                error_message=result.get("error"),
                user=user,
                metadata={
                    'function': 'analyze_backtest_trades_with_ai',
                    'error': result.get("error"),
                    'provider_attempts': result.get("provider_attempts"),
                }
            )
        except Exception:
            pass

    return result


def generate_basic_backtest_analysis(
    backtest_results: Dict[str, Any], 
    strategy: Dict[str, Any], 
    symbol: str,
    data_provider: str = None,
    data_points: int = 0,
    date_range: str = None
) -> str:
    """Generate a basic backtest analysis without AI - fallback when Gemini is not available"""
    total_trades = backtest_results.get('total_trades', 0)
    winning_trades = backtest_results.get('winning_trades', 0)
    losing_trades = backtest_results.get('losing_trades', 0)
    total_return = backtest_results.get('total_return', 0.0)
    win_rate = backtest_results.get('win_rate', 0.0)
    max_drawdown = backtest_results.get('max_drawdown', 0.0)
    
    entry_conditions = strategy.get('entry_conditions', [])
    exit_conditions = strategy.get('exit_conditions', [])
    
    analysis = f"ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ Ø¨Ú©â€ŒØªØ³Øª Ø¨Ø±Ø§ÛŒ {symbol}\n\n"
    
    # Add data source information at the beginning
    if data_provider or data_points > 0 or date_range:
        analysis += "ğŸ“Š Ù…Ù†Ø§Ø¨Ø¹ Ø¯Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡:\n"
        if data_provider:
            analysis += f"â€¢ Ø§Ø±Ø§Ø¦Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ø¯Ø§Ø¯Ù‡: {data_provider}\n"
        if date_range:
            analysis += f"â€¢ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ: {date_range}\n"
        if data_points > 0:
            analysis += f"â€¢ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ø¯Ø§Ø¯Ù‡: {data_points:,}\n"
        analysis += "\n"
    
    analysis += "=" * 80 + "\n\n"
    
    if total_trades > 0:
        analysis += f"ğŸ“ˆ Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ:\n"
        analysis += f"- ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª: {total_trades}\n"
        analysis += f"- Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨Ø±Ù†Ø¯Ù‡: {winning_trades}\n"
        analysis += f"- Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨Ø§Ø²Ù†Ø¯Ù‡: {losing_trades}\n"
        analysis += f"- Ù†Ø±Ø® Ø¨Ø±Ø¯: {win_rate:.2f}%\n\n"
        
        if winning_trades > losing_trades:
            analysis += f"âœ… Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø«Ø¨ØªÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª. {winning_trades} Ù…Ø¹Ø§Ù…Ù„Ù‡ Ø¨Ø±Ù†Ø¯Ù‡ Ø¯Ø± Ù…Ù‚Ø§Ø¨Ù„ {losing_trades} Ù…Ø¹Ø§Ù…Ù„Ù‡ Ø¨Ø§Ø²Ù†Ø¯Ù‡.\n\n"
        elif losing_trades > winning_trades:
            analysis += f"âš ï¸ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ø§Ø±Ø¯. {losing_trades} Ù…Ø¹Ø§Ù…Ù„Ù‡ Ø¨Ø§Ø²Ù†Ø¯Ù‡ Ø¯Ø± Ù…Ù‚Ø§Ø¨Ù„ {winning_trades} Ù…Ø¹Ø§Ù…Ù„Ù‡ Ø¨Ø±Ù†Ø¯Ù‡.\n\n"
        else:
            analysis += f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨Ø±Ù†Ø¯Ù‡ Ùˆ Ø¨Ø§Ø²Ù†Ø¯Ù‡ Ø¨Ø±Ø§Ø¨Ø± Ø§Ø³Øª.\n\n"
        
        if total_return > 0:
            analysis += f"ğŸ’¹ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒØŒ Ø³Ø±Ù…Ø§ÛŒÙ‡ Ø´Ù…Ø§ {total_return:.2f}% Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª.\n\n"
        elif total_return < 0:
            analysis += f"ğŸ“‰ Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø¨Ø§ Ø§ÛŒÙ† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒØŒ Ø³Ø±Ù…Ø§ÛŒÙ‡ Ø´Ù…Ø§ {abs(total_return):.2f}% Ú©Ø§Ù‡Ø´ ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª.\n\n"
        else:
            analysis += f"â¡ï¸ Ø§ÛŒÙ† Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø³ÙˆØ¯ ÛŒØ§ Ø¶Ø±Ø± Ø®Ø§ØµÛŒ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª.\n\n"
    else:
        analysis += "âš ï¸ Ù‡ÛŒÚ† Ù…Ø¹Ø§Ù…Ù„Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ø¨Ú©â€ŒØªØ³Øª Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª. Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø´Ø±Ø§ÛŒØ· ÙˆØ±ÙˆØ¯ ÛŒØ§ Ø®Ø±ÙˆØ¬ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù‡Ù…Ø®ÙˆØ§Ù†ÛŒ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯.\n\n"
    
    # Strategy analysis
    if entry_conditions or exit_conditions:
        analysis += f"ğŸ“‹ ØªØ­Ù„ÛŒÙ„ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ:\n"
        analysis += f"- ØªØ¹Ø¯Ø§Ø¯ Ø´Ø±Ø§ÛŒØ· ÙˆØ±ÙˆØ¯: {len(entry_conditions)}\n"
        analysis += f"- ØªØ¹Ø¯Ø§Ø¯ Ø´Ø±Ø§ÛŒØ· Ø®Ø±ÙˆØ¬: {len(exit_conditions)}\n\n"
        
        if entry_conditions:
            analysis += "Ø´Ø±Ø§ÛŒØ· ÙˆØ±ÙˆØ¯:\n"
            for idx, cond in enumerate(entry_conditions[:5], 1):
                analysis += f"  {idx}. {cond[:100]}...\n"
            analysis += "\n"
        
        if exit_conditions:
            analysis += "Ø´Ø±Ø§ÛŒØ· Ø®Ø±ÙˆØ¬:\n"
            for idx, cond in enumerate(exit_conditions[:5], 1):
                analysis += f"  {idx}. {cond[:100]}...\n"
            analysis += "\n"
    
    # Trade analysis if available
    trades = backtest_results.get('trades', [])
    if trades:
        profits = [t.get('profit', 0) for t in trades if t.get('profit', 0) > 0]
        losses = [t.get('profit', 0) for t in trades if t.get('profit', 0) < 0]
        
        if profits:
            avg_profit = sum(profits) / len(profits)
            analysis += f"ğŸ’° Ù…ØªÙˆØ³Ø· Ø³ÙˆØ¯ Ù‡Ø± Ù…Ø¹Ø§Ù…Ù„Ù‡ Ø¨Ø±Ù†Ø¯Ù‡: {avg_profit:.2f}\n"
        
        if losses:
            avg_loss = abs(sum(losses) / len(losses))
            analysis += f"ğŸ“‰ Ù…ØªÙˆØ³Ø· Ø¶Ø±Ø± Ù‡Ø± Ù…Ø¹Ø§Ù…Ù„Ù‡ Ø¨Ø§Ø²Ù†Ø¯Ù‡: {avg_loss:.2f}\n"
    
    analysis += "\n" + "=" * 80
    analysis += "\n\nğŸ’¡ ØªÙˆØµÛŒÙ‡: Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ù‡Ø± Ø´Ø±Ø· ÙˆØ±ÙˆØ¯/Ø®Ø±ÙˆØ¬ØŒ Ø§Ø² ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯."
    
    return analysis


def generate_ai_recommendations(
    parsed_strategy: Dict[str, Any],
    raw_text: str = None,
    analysis: Dict[str, Any] = None
) -> Dict[str, Any]:
    """
    Generate AI recommendations for improving the strategy
    Each recommendation costs 150,000 Toman
    """
    # Prepare strategy information
    strategy_info = f"""
    Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ:
    - Ø´Ø±Ø§ÛŒØ· ÙˆØ±ÙˆØ¯: {', '.join(parsed_strategy.get('entry_conditions', []))}
    - Ø´Ø±Ø§ÛŒØ· Ø®Ø±ÙˆØ¬: {', '.join(parsed_strategy.get('exit_conditions', []))}
    - Ù…Ø¯ÛŒØ±ÛŒØª Ø±ÛŒØ³Ú©: {json.dumps(parsed_strategy.get('risk_management', {}), ensure_ascii=False)}
    - Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§: {', '.join(parsed_strategy.get('indicators', []))}
    - Ù†Ù…Ø§Ø¯: {parsed_strategy.get('symbol', 'ØªØ¹ÛŒÛŒÙ† Ù†Ø´Ø¯Ù‡')}
    - ØªØ§ÛŒÙ…â€ŒÙØ±ÛŒÙ…: {parsed_strategy.get('timeframe', 'ØªØ¹ÛŒÛŒÙ† Ù†Ø´Ø¯Ù‡')}
    """
    
    if raw_text:
        strategy_info += f"\nÙ…ØªÙ† Ø§ØµÙ„ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ:\n{raw_text[:2000]}"
    
    if analysis:
        strategy_info += f"\n\nØªØ­Ù„ÛŒÙ„ ÙØ¹Ù„ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ:\n{json.dumps(analysis, ensure_ascii=False)}"
    
    prompt = f"""
    Ø´Ù…Ø§ ÛŒÚ© Ù…Ø´Ø§ÙˆØ± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù…Ø¹Ø§Ù…Ù„Ø§ØªÛŒ Ù‡Ø³ØªÛŒØ¯. Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø²ÛŒØ±ØŒ 3-5 Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¹Ù…Ù„ÛŒ Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.
    
    {strategy_info}
    
    Ù‡Ø± Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¨Ø§ÛŒØ¯:
    1. ÛŒÚ© Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ø¶Ø­ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
    2. ØªÙˆØ¶ÛŒØ­ Ú©Ø§Ù…Ù„ÛŒ Ø§Ø² Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡Ø¯
    3. Ù†ÙˆØ¹ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†Ø¯ (entry_condition, exit_condition, risk_management, indicator, parameter, general)
    4. Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
    
    Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§ÛŒØ¯ JSON Ø¨Ø§ Ø§ÛŒÙ† Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø§Ø´Ø¯:
    {{
      "recommendations": [
        {{
          "title": "Ø¹Ù†ÙˆØ§Ù† Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯",
          "description": "ØªÙˆØ¶ÛŒØ­ Ú©Ø§Ù…Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯",
          "type": "entry_condition|exit_condition|risk_management|indicator|parameter|general",
          "data": {{
            "suggested_value": "Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ",
            "implementation": "Ø±ÙˆØ´ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ",
            "expected_improvement": "Ø¨Ù‡Ø¨ÙˆØ¯ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±"
          }}
        }}
      ]
    }}
    
    ÙÙ‚Ø· JSON Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯ØŒ Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­Ø§Øª Ø§Ø¶Ø§ÙÛŒ.
    """
    
    cache_key = f"{strategy_info}\n{analysis}"

    def _parse_response(raw_output: str) -> Dict[str, Any]:
        if not raw_output:
            return _build_base_response(
                ai_status="error",
                message=SERVICE_UNAVAILABLE_MESSAGE,
                raw_output=raw_output,
                extra={"recommendations": []}
            )
        try:
            data: Dict[str, Any] = json.loads(raw_output)
        except json.JSONDecodeError as exc:
            logger.error("Error parsing AI recommendations JSON: %s", exc)
            return _build_base_response(
                ai_status="error",
                message=SERVICE_UNAVAILABLE_MESSAGE,
                raw_output=raw_output,
                extra={"recommendations": [], "error": str(exc)}
            )

        recommendations = data.get("recommendations") or []
        if not isinstance(recommendations, list):
            return _build_base_response(
                ai_status="error",
                message=SERVICE_UNAVAILABLE_MESSAGE,
                raw_output=raw_output,
                extra={"recommendations": [], "error": "invalid_structure"}
            )

        return _build_base_response(
            ai_status="ok",
            message="AI recommendations generated.",
            raw_output=raw_output,
            extra={
                "recommendations": recommendations,
                "recommendations_count": len(recommendations),
                "is_basic": False
            }
        )

    return _call_gemini(
        prompt,
        cache_namespace="recommendations",
        cache_key=cache_key,
        generation_config={
            'temperature': 0.8,
            'response_mime_type': 'application/json',
            'provider_metadata': {'system_prompt': JSON_ONLY_SYSTEM_PROMPT},
        },
        response_parser=_parse_response
    )
